# Hyperparameters for DDPG
import dl.examples.ddpg

train.Trainer = @DDPG

optim.Adam.betas = (0.9, 0.999)

DDPG.maxt = 1000000
DDPG.seed = 0
DDPG.eval = True
DDPG.eval_period = 100000
DDPG.save_period = 100000
DDPG.maxseconds = None
DDPG.gpu = True

DDPG.env_fn = @make_env
DDPG.policy_fn = @policy_fn
DDPG.qf_fn = @qf_fn
DDPG.nenv = 1
DDPG.norm_observations = True
DDPG.eval_num_episodes = 20
DDPG.record_num_episodes = 5

DDPG.buffer_size = 100000
DDPG.frame_stack = 1
DDPG.learning_starts = 50000
DDPG.update_period = 1

DDPG.env_fn = @make_env
DDPG.optimizer = @optim.Adam
DDPG.batch_size = 128
DDPG.policy_lr = 1e-4
DDPG.qf_lr = 1e-3
DDPG.qf_weight_decay = 1e-2
DDPG.gamma = 0.99
DDPG.noise_theta = 0.15
DDPG.noise_sigma = 0.2
DDPG.noise_sigma_final = 0.01
DDPG.noise_decay_period = 100000
DDPG.target_update_period = 1
DDPG.target_smoothing_coef = 0.001
DDPG.reward_scale = 1
DDPG.log_period = 100

Checkpointer.ckpt_period = 10000

make_env.env_id = "LunarLanderContinuous-v2"

ObsNorm.steps = 10000
ObsNorm.mean = None
ObsNorm.std = None
ObsNorm.eps = 1e-2
ObsNorm.log = True
ObsNorm.log_prob = 0.01
